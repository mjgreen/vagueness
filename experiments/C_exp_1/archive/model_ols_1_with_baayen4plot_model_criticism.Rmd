---
output:
  html_document:
    toc: true
    <!--toc_float: true-->
    mathjax: default
    template: default
    code_folding: hide
    <!--theme: default-->
---

## ols model v4, RecSqt, in progress

```{r}
load("dd.Rda")
suppressPackageStartupMessages(require(rms))
suppressPackageStartupMessages(require(languageR))
options(width=140)
```

Pairs plot

```{r, fig.width=7, fig.height=7}
# pairs plot
pairs(dd[,c("RT_RecSqt", "Number", "Vagueness", "RTprev_RecSqt")], pch = 19, cex=.075)
```

Build the model

```{r}
## do ols model
# define datadist
dd.dd = datadist(dd)
options(datadist = "dd.dd")
# build model
v4 <- ols(data=dd, 
          RT_RecSqt ~ 
            c_Vag + c_Num + c_Qty + c_Ord + 
            c_Num:c_Vag:c_Qty+
            item_mean_ratio + 
            #c_Vag:c_Num + 
            #c_Vag:c_Num:item_mean_ratio + 
            # pol(s_Trl, 3) +
            s_Trl+
            # pol(RTprev_RecSqt,3) +
            RTprev_RecSqt +
            nchar_instr
            # cell+
            # measurement,
            ,
          x=T, y=T )
```

```{r}
# print out p value for likelihood ratio with df
cat("lik.ratio p value, whether the model as a whole is explanatory: p =", 1-pchisq(v4$stats[2],v4$stats[3]))
# print out R2 for the model
cat("R^2 = ", v4$stats[4])
```

Show the model tables

```{r}
# show model
v4
# show model summary
summary(v4)
# do anova 
an1 = anova(v4)
# show anova
an1
```

plot 'partial effects'

```{r, fig.width=12, fig.height=5}
## plot partial effects
# Compute Predicted Values and Confidence Limits
p1=Predict(v4)
# plot 'partial effects'
plot(p1, anova=an1, pval=T, aspect=1, main="ols model v4")
```

```{r}
# add predicted RT_RecSqt to dd
dd$RT_Predicted <- predict (v4)
```



Plot model coefficients and ci's

```{r}
par(las=2, mar=c(14,6,1,1))
y=coef(v4) # with intercept
n=length(y)
y0=confint(v4)[1:n,1]
y1=confint(v4)[1:n,2]

y=coef(v4)[-1] # omit intercept
n=length(y)
y0=confint(v4)[-1,1]
y1=confint(v4)[-1,2]

plot(y, xaxt="n", xlab="", ylab="RT_RecSqt\n", pch=19, ylim=extendrange(y, f=.10), main="Speed")
abline(h=0)
axis(1, labels=names(y) , at=1:n)
grid()
segments(x0=1:n, x1=1:n, y0, y1, lwd=2)
```

Collinearity is assessed by the condition number $k$.\
The greater the collinearity, the closer the matrix of predictors is is to becoming _singular_.\
$k$ estimates the extent to which a matrix is $\text{singular}$\
$0\ldots6$ no collinearity\
$\text{around} 15$ medium collinearity\
$>30$ indicates potentially harmful collinearity\

```{r}
cat("k is:",
collin.fnc(dd[,c("s_Trl", "c_Num", "c_Vag", "c_Qty", "c_Ord", "item_mean_ratio", "RTprev_RecSqt", "nchar_instr", "measurement")])$cnumber
)
```

```{r, fig.width=6, fig.height=5}
par(mar=c(1.1,3.1,1.1,1.1), pty='s')
plot(varclus(as.matrix(dd[,c("s_Trl", "c_Num", "c_Vag", "c_Qty", "c_Ord", "item_mean_ratio", "RTprev_RecSqt", "nchar_instr", "measurement")])))
```

Model criticism baayen plots

```{r, fig.width=9, fig.height=3}
# Baayen 4-plot model criticism

par(mfrow=c(1,4), pty='s')
# create scaled residuals
v4$rstand = as.vector(scale(resid(v4)))
# plot scaled residuals density
plot(density(v4$rstand))
# plot sample quantiles versus theoretical quantiles
qqnorm(v4$rstand, cex=.5)
qqline(v4$rstand)
# plot standardised residuals versus fitted values
plot(v4$rstand ~ fitted(v4), pch='.')
# absolute standardised residuals greater than 2.5 are candidates for being outliers, the abline identifies them on the plot
abline(h=c(-2.5,2.5))
# create dffits
dffits=abs(resid(v4, "dffits"))
# plot dffits
plot(dffits, type='h')
```


```{r}
# here no overly influential
w = which.influence(fit = v4, cutoff = 0.4)
w
```

Model validation

Validation tests overfitting using bootstrap.

Bootstrapping draws as a bootstrap sample the same number of samples as the original data, at random with replacement, from the original data.

Then fit the model to the data in the bootstrap sample, and use this model to predict the reaction times for the original full data (which contains many samples that are new to the bootstrap model).

Then compare the goodness of fit of the bootstrap model with the goodness of fit of the original model.

Averaged over a large number of bootstrap models these comparisons of goodness of fit  reveal to what extent the original model overfits the data.

Function validate() in rms does this _re-sampling validation_.


```{r, val_1, echo=TRUE}
# argument B is the number of bootstrap runs
# argument pr is whether to print the results of each run
v_1 <- validate(v4, bw = T, B = 200, pr=FALSE, estimates=TRUE)
```

```{r}
# in print.validate, B is the number of re-samples to show outocmes for each resample
print(v_1, B=0)
```

```{r}
# show how often each number of variables kept was observed across the 200 runs
xtabs(~rowSums(attr(v_1,"kept")))
```

