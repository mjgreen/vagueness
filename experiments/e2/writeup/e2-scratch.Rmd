
```{r setup, include=FALSE, cache=FALSE}
library(knitr)
```

```{r loadFonts, message=FALSE, echo=FALSE}
library(fontcm)
library(extrafont)
#font_import()
#loadfonts()
```

```{r loadLibs, message=FALSE, echo=FALSE}
library(beepr)                     #
library(car)                       #
library(ggplot2)                   #
library(grid)                      #
library(gridExtra)                 #
library(languageR)                 #
library(lattice)                   #
library(latticeExtra)              # for doubleYScale
library(lme4)                      #
library(lmerTest)                  #
library(LMERConvenienceFunctions)  # for perSubjectTrim
library(MASS)                      #
library(plyr)                      #
library(reshape2)                  #
library(rms)                       # for varclus
library(Rmisc)                     # for summarySEwithin and friends
library(xtable)                    #
```

```{r preprocessing}
source('e2-preprocessing.R')
if( !file.exists('e2-b-data.txt') ) {
  dat <- gatherData() 
  dat <- declareImpossibleRT(dat)
  dat <- removeImpossibleTrials(dat)
  dat <- subset(dat, select=c(Subject, Trial, Item, 
                              discriminability, 
                              Instruction,
                              nchar_instr,
                              Vagueness, Number, Order, Quantity, 
                              c_Vag, c_Num, c_Ord, c_Qty,
                              NaType, response_category, 
                              isBorderline, RT)
  )
  write.table(dat, file='e2-b-data.txt', sep='\t', quote=FALSE)
}
dd <- read.delim('e2-b-data.txt')
```

```{r showCompression, fig.width=6, fig.height=4, fig.pos='hbtp', fig.cap='Ratios for different numbers of dots in the arrays: smaller values are more discriminable. Blue is for the ratio between the smallest number in the array and the largest number in an array. Red is for the mean of two ratios, one for the smallest number to the middle number, the other for the middle number to the largest number in the array', echo=FALSE}
#  Show compression of ratios of numbers of dots in the items
discriminability = sort(unique(as.numeric(dd$discriminability)))
plot(y=c(1.5,1.5,1.5,1.5), 
     x=discriminability, 
     ylab='',
     xlab='',
     yaxt='n',
     type='n', 
     xaxt='n',
     axes=F)
lines(y=c(1.5,1.5,1.5,1.5), x=discriminability, type='o', pch=19)
text(y=c(1.15,1.15,1.15,1.15), x=discriminability, labels=round(discriminability,2),  cex=.7)
mtext('mean (ratio min/med,\nratio med/max)', side = 2, line = 1)
mtext('Discriminability index', side=1, line=3)
mtext('Discriminability for each item', side=3, line=1, cex=1.5)
axis(1)
box(which='plot', col='grey')
```

```{r e2-rtplot, fig.width=7, fig.height=3.5, echo=FALSE, message=FALSE, dev='pdf'}
dd$RT_log=log(dd$RT)
rtdata=dd
source('summarySEwithin2.R')
rtdataplot1 <- summarySEwithin2(rtdata, measurevar="RT_log", 
                                withinvars=c("Vagueness", "Number", "Item"), idvar="Subject")
rtdataplot1$Condition <- as.factor(paste(sep=':', rtdataplot1$Number, rtdataplot1$Vagueness))
rtplot=ggplot(rtdataplot1, aes(y=RT_logNormed, x=Item, group=Condition, fill=Vagueness)) +
  geom_line() +
  geom_errorbar(width=.2, aes(ymin=RT_log-ci, ymax=RT_log+ci)) +
  geom_point(size=2, col=1, aes(shape=Vagueness)) +
  scale_fill_grey(name="Vagueness", start=0, end=1) +
  scale_shape_manual(name="Vagueness", values=c(21,22)) +
  ggtitle("Response time") +
  ylab("Respone time (log (ms))") + 
  xlab(NULL) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.key = element_blank(), legend.position="top",
        aspect.ratio=1,
        plot.background=element_rect(fill=NA, color='white'),
        strip.background=element_blank(),
        axis.text.x = element_text(angle = 15)) +
  facet_grid(~Number)
rtplot
```
#Lmer model: before outlier removal

```{r}
dd$RT_log <- log(dd$RT)
dd$c_Trl <- as.numeric(scale(dd$Trial))
```


```{r lmer5, tidy=FALSE, cache=TRUE}
vlmerTest <- lmerTest::lmer(data=dd,
                 RT_log ~
                   c_Vag + c_Num + c_Qty + c_Ord +
                   c_Num:c_Vag:c_Qty +
                   discriminability +
                   c_Trl +
                   #RTprev_log +
                   nchar_instr +
                   (1+c_Vag + c_Num + c_Qty + c_Ord|Subject))
```



```{r printXtableV5, echo=FALSE, results='asis'}
print(
  xtable(
    summary(vlmerTest)$coef,
    caption = 'lmerTest version',
    digits=c(0, 3,3,1,1,4)),
  type='html',
  caption.placement='top'
)
```


```{r llmer5, tidy=FALSE, cache=TRUE}
vlme4 <- lme4::lmer(data=dd,
                 RT_log ~
                   c_Vag + c_Num + c_Qty + c_Ord +
                   c_Num:c_Vag:c_Qty +
                   discriminability +
                   c_Trl +
                   #RTprev_log +
                   nchar_instr +
                   (1+c_Vag + c_Num + c_Qty + c_Ord|Subject))
```

```{r printXtableV521c, echo=FALSE, results='asis'}
print(
  xtable(
    summary(vlme4)$coef,
    caption = 'lme4 version',
    digits=c(0, 3,3,1)),
  type='html',
  caption.placement='top'
)
```




```{r print_info_on_design}
cat(paste('number of subjects = ',30,'\n',
          '---','\n',
          'number of trials per subject = ',256,'\n', 
          '---','\n',          
          'number of unique cells in the design = Item(4) * Number(2) * Vagueness(2) * Order(2) * Quantity(2) = ',4*2*2*2*2,'\n',
          'number of times a subject saw a unique cell in the design = 256 / 64 = ', 256/64,'\n',
          '---','\n',
          'number of higher-level cells in the design (abstracting over Order and Quantity) = Item(4) * Number(2) * Vagueness(2) = ',4*2*2,'\n',
          'number of times a subject saw a higher level cell in the design =  = 256 / 16 = ', 4*2*2, '\n',
          sep=''))
```

# Distributional stuff, including transformations.

```{r transformations_table, results='asis'}
x=445
lambda = c(-2,-1,-0.5, 0,0.5,1,2)

xd=data.frame(
  lambda = c(
    lambda[1],
    lambda[2],
    lambda[3],
    lambda[4],
    lambda[5],
    lambda[6],
    lambda[7]
  ),
  x=c(    
    paste("x = ", sep="", x),
    paste("x = ", sep="", x),
    paste("x = ", sep="", x),
    paste("x = ", sep="", x),
    paste("x = ", sep="", x),
    paste("x = ", sep="", x),
    paste("x = ", sep="", x)
  ),
  conventional=c(
    '$$y=\\frac{1}{x^2}$$', 
    '$$y=\\frac{1}{x}$$', 
    '$$y=\\frac{1}{\\sqrt{x}}$$', 
    '$$y=log_e(x)$$', 
    '$$y=\\sqrt{x}$$', 
    '$$y=x$$', 
    '$$y=x^2$$'
  ),
  power=c(
    '$$y=x^{-2}$$', 
    '$$y=x^{-1}$$', 
    '$$y=x^{-0.5}$$', 
    '$$y=log_e(x)$$', 
    '$$y=x^{0.5}$$', 
    '$$y=x^{1}$$', 
    '$$y=x^{2}$$'
  ),
  y=c(     
    paste("y = ", sep="", format(round(x^lambda[1],6), sc=F) ),
    paste("y = ", sep="", round(x^lambda[2],3) ),
    paste("y = ", sep="", round(x^lambda[3],3) ),
    paste("y = ", sep="", round(log(x),3) ),
    paste("y = ", sep="", round(x^lambda[5],2) ),
    paste("y = ", sep="", round(x^lambda[6],6) ),
    paste("y = ", sep="", round(x^lambda[7],6) )
  ),
  back = c(
    '$$x=y^{\\frac{1}{-2}}$$',
    '$$x=y^{\\frac{1}{-1}}$$', 
    '$$x=y^{\\frac{1}{-0.5}}$$', 
    '$$x=exp(y)$$',  
    '$$x=y^{\\frac{1}{0.5}}$$', 
    '$$x=y^{\\frac{1}{1}}$$',  
    '$$x=y^{\\frac{1}{2}}$$'
  ),
  back_lambda=c(
    '$$x=y^{1/\\lambda}$$',
    '$$x=y^{1/\\lambda}$$',
    '$$x=y^{1/\\lambda}$$',
    '$$x=exp(y)$$',  
    '$$x=y^{1/\\lambda}$$',
    '$$x=y^{1/\\lambda}$$',
    '$$x=y^{1/\\lambda}$$'
  ),
  y_back=c(  
    paste( 'x = ', (x^lambda[1]) ^ {1/lambda[1]} ,sep=""),
    paste( 'x = ', (x^lambda[2]) ^ {1/lambda[2]} ,sep=""),
    paste( 'x = ', (x^lambda[3]) ^ {1/lambda[3]} ,sep=""),
    paste( 'x = ', exp(log(x)), sep=""),
    paste( 'x = ', (x^lambda[5]) ^ {1/lambda[5]} ,sep=""),
    paste( 'x = ', (x^lambda[6]) ^ {1/lambda[6]} ,sep=""),
    paste( 'x = ', (x^lambda[7]) ^ {1/lambda[7]} ,sep="")
  )
) 

print(xtable(xd, digits=1, align=c(rep('l', times=ncol(xd)+1))), type='html', include.rownames=F, digits=1)
```

Show practice and fatigue effects vary over items.

```{r pracFatigueItms, eval=TRUE, fig.width=7, fig.height=3.5, echo=FALSE}
# Show how practice/fatigue effects vary over items
ggplot(dd, aes(y=RT_RecSqt, x=Trial)) + 
  facet_grid(~Item) + 
  theme(panel.grid=element_blank(), panel.margin = unit(0, "lines"), 
        panel.background = element_rect(fill = 'white', colour='black' ), 
        legend.position="top") + 
  scale_x_continuous("Trial") + 
  geom_point(pch=19, cex=.25, alpha=.5) + 
  stat_smooth(fill='lightblue',col="red",lwd=.5, show.legend=TRUE) + 
  theme(aspect.ratio = 1) 
```

## ols model v4, RecSqt, in progress

Pairs plot

```{r pairs, fig.width=7, fig.height=7}
 pairs(dd[,c("RT_RecSqt", "Number", "Vagueness", "RTprev_RecSqt")], pch = 19, cex=.075)
```

Build the model

```{r doModelols}
## do ols model
# define datadist
dd.dd = datadist(dd)
options(datadist = "dd.dd")
# build model
v4 <- ols(data=dd, 
          RT_RecSqt ~ 
            c_Vag + c_Num + c_Qty + c_Ord + 
            c_Num:c_Vag:c_Qty+
            item_mean_ratio + 
            #c_Vag:c_Num + 
            #c_Vag:c_Num:item_mean_ratio + 
            # pol(s_Trl, 3) +
            s_Trl+
            # pol(RTprev_RecSqt,3) +
            RTprev_RecSqt +
            nchar_instr
          # cell+
          # measurement,
          ,
          x=T, y=T )
```

```{r likRat}
# print out p value for likelihood ratio with df
cat("lik.ratio p value, whether the model as a whole is explanatory: p =", 1-pchisq(v4$stats[2],v4$stats[3]))
# print out R2 for the model
cat("R^2 = ", v4$stats[4])
```

Show the model tables

```{r modTabs}
# show model
v4
# show model summary
summary(v4)
# do anova 
an1 = anova(v4)
# show anova
an1
```

plot 'partial effects'

```{r partial, fig.width=12, fig.height=5}
## plot partial effects
# Compute Predicted Values and Confidence Limits
p1=Predict(v4)
# plot 'partial effects'
plot(p1, anova=an1, pval=T, aspect=1, main="ols model v4")
```

```{r daz99}
# add predicted RT_RecSqt to dd
dd$RT_Predicted <- predict (v4)
```

Plot model coefficients and ci's

```{r coefsAndCiS}
par(las=2, mar=c(14,6,1,1))
y=coef(v4) # with intercept
n=length(y)
y0=confint(v4)[,1]
y1=confint(v4)[,2]
y=coef(v4)[-1] # omit intercept
n=length(y)
y0=confint(v4)[-1,1]
y1=confint(v4)[-1,2]
plot(y, xaxt="n", xlab="", ylab="RT_RecSqt\n", pch=19, ylim=extendrange(y, f=.10), main="Speed")
abline(h=0)
axis(1, labels=names(y) , at=1:n)
grid()
segments(x0=1:n, x1=1:n, y0, y1, lwd=2)
```

Collinearity is assessed by the condition number $k$.\
The greater the collinearity, the closer the matrix of predictors is is to becoming _singular_.\
$k$ estimates the extent to which a matrix is $\text{singular}$\
$0\ldots6$ no collinearity\
$\text{around} 15$ medium collinearity\
$>30$ indicates potentially harmful collinearity\

```{r kis}
cat("k is:",
collin.fnc(dd[,	c("s_Trl", "c_Num", "c_Vag", "c_Qty", "c_Ord", "item_mean_ratio", 
"RTprev_RecSqt", "nchar_instr", "measurement")
])$cnumber
)
```

```{r plotg, fig.width=6, fig.height=5}
par(mar=c(1.1,3.1,1.1,1.1), pty='s')
plot(varclus(
as.matrix(dd[,c("s_Trl", "c_Num", "c_Vag", "c_Qty", "c_Ord",
"item_mean_ratio", "RTprev_RecSqt", "nchar_instr", "measurement")])))
```

Model criticism baayen plots

```{r baayenMplots, fig.width=9, fig.height=3}
# Baayen 4-plot model criticism
par(mfrow=c(1,4), pty='s')
# create scaled residuals
v4$rstand = as.vector(scale(resid(v4)))
# plot scaled residuals density
plot(density(v4$rstand), main='density of\nscaled residuals')
# plot sample quantiles versus theoretical quantiles
qqnorm(v4$rstand, cex=.5)
qqline(v4$rstand)
# plot standardised residuals versus fitted values
plot(v4$rstand ~ fitted(v4), pch='.', main='std resid\nv fitted')
# absolute standardised residuals greater than 2.5 are candidates 
# for being outliers, the abline identifies them on the plot
abline(h=c(-2.5,2.5))
# create dffits
dffits=abs(resid(v4, "dffits"))
# plot dffits
plot(dffits, type='h', main='dffits')
# here no overly influential
w = which.influence(fit = v4, cutoff = 0.4)
if (length(w)==0) {
print('There are no overly influential data points')
} else {
print('There are some overly influential variables:');
print(w)
}
```

Model validation

Validation tests overfitting using bootstrap.

Bootstrapping draws as a bootstrap sample the same number of samples as the original data, at random with replacement, from the original data.

Then fit the model to the data in the bootstrap sample, and use this model to predict the reaction times for the original full data (which contains many samples that are new to the bootstrap model).

Then compare the goodness of fit of the bootstrap model with the goodness of fit of the original model.

Averaged over a large number of bootstrap models these comparisons of goodness of fit  reveal to what extent the original model overfits the data.

Function validate() in rms does this _re-sampling validation_.


```{r val_1, echo=TRUE}
# argument B is the number of bootstrap runs
# argument pr is whether to print the results of each run
v_1 <- validate(v4, bw = T, B = 200, pr=FALSE, estimates=TRUE)
```

```{r prval}
# in print.validate, B is the number of re-samples to show outocmes for each resample
print(v_1, B=0)
```

```{r kept}
# show how often each number of variables kept was observed across the 200 runs
xtabs(~rowSums(attr(v_1,"kept")))
```

#FONTS

```{r defineFonts}
.define.fonts <- function () {
quartzFonts(avenir =    c('Avenir Book', 'Avenir Black', 'Avenir Book Oblique', 'Avenir Black Oblique'),
helvetica = c('Helvetica New Light', 'Helvetica New Bold', 'Helvetica New Light Italic', 'Helvetica New Bold Italic'),
cmodern = c('CMU Serif', 'CMU Serif Extra','CMU Classical Serif','CMU Serif Extra'))
}
# .define.fonts()
```


```{r exampleOfGGplotFonts, fig.width=3, fig.height=3, fig.cap='ggplot special font example', echo=FALSE}
# ggplot special fonts
p <- ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point() +
ggtitle("Fuel Efficiency of 32 Cars") +
xlab("Weight (x1000 lb)") + ylab("Miles per Gallon") +
theme_bw() +
theme(text=element_text(family="CMU Serif", size=14, face='plain'))
p
ggsave("ggplot_CMU_Serif.pdf", p, width=3.5, height=3.5)
```

```{r exampleOfBaseGraphicsFonts, fig.width=3, fig.height=3, fig.cap='base graphic special font example'}
# base graphic special font
pdf("plot_cm.pdf", family="CM Roman", width=5.5, height=5)
curve(dnorm, from=-3, to=3, main="Normal Distribution")
text(x=0, y=0.1, cex=1.5, expression(italic(y == frac(1, sqrt(2 * pi)) *
e ^ {-frac(x^2, 2)} )))
dev.off()
embed_fonts("plot_cm.pdf", outfile="plot_cm_embed.pdf")
```

```{r howManyCellsInDesign}
# number of higher-level cells in the design (abstracting over Order) =
# Item(4) * Number(2) * Vagueness(2) * Quantity(2) = 32
# number of measurements in eaach cell = 8
# 332 * 8 = 256
# 256 * 30 = 7680
dd$measurement=0
dd$cell=0
for (s in levels(dd$Subject)) {
  cellcount=0
  for (i in levels(dd$Item)) {
    for (n in levels(dd$Number)) {
      for (v in levels(dd$Vagueness)) {
        for (q in levels(dd$Quantity)) {
          measurementcount=0
          cellcount=cellcount+1
          for (t in dd[dd$Subject==s & dd$Item==i & dd$Number==n & dd$Vagueness==v & dd$Quantity == q, "Trial"]) {
            dd[dd$Subject==s & dd$Item==i & dd$Number==n & dd$Vagueness==v & dd$Quantity == q & dd$Trial==t, "cell"] <- cellcount
            measurementcount=measurementcount+1
            dd[dd$Subject==s & dd$Item==i & dd$Number==n & dd$Vagueness==v & dd$Quantity == q & dd$Trial==t, "measurement"] = measurementcount
          }
        }
      }
    }
  }
}
```




```{r rSquaredInfo, echo=FALSE}
cat("R^2")
cor(fitted(v5), dd$RT_log)^2
```

```{r plotModelCoefsAndCis, fig.width=5.5, fig.height=4, fig.cap='Coefficient estimates and their (Wald) 95 per cent confidence intervals', fig.pos='hbtp', echo=FALSE}
# Plot model coefficients and ci's
dfr <- data.frame(coef = summary(v5)$coef[-1, 1], 
                  ci2.5  = confint(v5, method='Wald')[18:26, 1],
                  ci97.5 = confint(v5, method='Wald')[18:26, 2] )
dfr <- dfr[rev(rownames(dfr)),]
par(mar=c(5,10,1,1))
plot(y=1:nrow(dfr), 
     x=dfr$coef, 
     xlim=range(dfr$ci2.5, dfr$ci97.5), 
     type='n', axes=F, xlab="", ylab="")
abline(v=0, lty=3, col='grey')
abline(h=c(1,2,3,4,5,6,7,8,9), col='grey', lty=3)
arrows(y0=1:nrow(dfr), y1=1:nrow(dfr), 
       x0=dfr$ci2.5, x1=dfr$ci97.5, 
       length=0.035,
       col='black',
       lwd=2, code=3, angle=90)
points(y=1:nrow(dfr), x=dfr$coef, pch=21, bg='white')
axis(2, labels=row.names(dfr), at=1:nrow(dfr), las=1, tick=FALSE)
axis(1)
mtext("Coefficient estimates, in units of log(RT),\nwith 95% confidence intervals", side=1, line=3.5, cex.lab=1, las=1)
mtext("Model terms", side=2, line=8)
box(which='plot', col='grey')

```




```{r plotLMERfnc, fig.width=6, fig.height=4, fig.pos='hbtp', fig.cap='plotMLERfnc', echo=TRUE}
par(mfrow=c(2,4))
plotLMER.fnc(v5)

```



```{r baayenPLots99, fig.width=6, fig.height=4, fig.cap='Baayen Model Criticism Plots', fig.pos='hbtp', echo=FALSE}
# Baayen 4-plot model criticism
par(mfrow=c(1,3), pty='s')
# create scaled residuals
dd$rstand = as.vector(scale(resid(v5)))
# plot scaled residuals density
plot(density(dd$rstand))
# plot sample quantiles versus theoretical quantiles
qqnorm(dd$rstand, cex=.5)
qqline(dd$rstand)
# plot standardised residuals versus fitted values
plot(dd$rstand ~ fitted(v5), pch='.')
# absolute standardised residuals greater than 2.5 are candidates for being outliers, the abline identifies them on the plot
abline(h=c(-2.5,2.5))

```



#lmerTest Version

```{r lmerTestVersionOfModel, tidy=FALSE, cache=TRUE, echo=TRUE}
v6 <- lmerTest::lmer(data=dd,
                     RT_log ~
                       c_Vag + c_Num + c_Qty + c_Ord +
                       c_Num:c_Vag:c_Qty +
                       discriminability +
                       s_Trl +
                       RTprev_log +
                       nchar_instr +
                       (1+c_Vag + c_Num + c_Qty + c_Ord|Subject))
```

```{r summaryLmerTest, cache=TRUE}
summary(v6)
```


#Lmer model: after outlier removal
not done yet.

#Borderline responses

```{r barplotBorderline, fig.width=6, fig.height=4, echo=FALSE}
dd$response_category <- relevel(dd$response_category, ref = 'expected')
barplot(table(dd$Vagueness,dd$response_category), beside=TRUE, legend=TRUE, space=c(0,.5))

```

```{r tabBdl, echo=FALSE, results='asis'}
print(xtable(table(dd$response_category,dd$Vagueness),
             caption='Borderline cases counts'),
      table.placement='htbp',
      latex.environments='center',
      size='small')
```

